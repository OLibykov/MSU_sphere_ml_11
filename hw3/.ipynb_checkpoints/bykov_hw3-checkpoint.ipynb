{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №3 - Дерево решений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 28 мая 2022, 08:30   \n",
    "**Штраф за опоздание:** по 1 баллу за 24 часа задержки.\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла Присылать ДЗ необходимо в виде ссылки на свой github репозиторий на почту ml1.sphere@mail.ru с указанием темы в следующем формате:\n",
    "[ML0221, Задание 3] Фамилия Имя. \n",
    "\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Реализуем дерево решений (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допишите недостающие части дерева решений. Ваша реализация дерева должна работать по точности не хуже DecisionTreeClassifier из sklearn.\n",
    "Внимание: если Вас не устраивает предложенная структура хранения дерева, Вы без потери баллов можете сделать свой класс DecisionTreeClassifier, в котором сами полностью воспроизведете алгоритм дерева решений. Обязательно в нем иметь только функции fit, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    NON_LEAF_TYPE = 0\n",
    "    LEAF_TYPE = 1\n",
    "\n",
    "    def __init__(self, min_samples_split=2, max_depth=5, criterion='gini'):\n",
    "        \"\"\"\n",
    "        criterion -- критерий расщепления. необходимо релизовать три:\n",
    "        Ошибка классификации, Индекс Джини, Энтропийный критерий\n",
    "        max_depth -- максимальная глубина дерева\n",
    "        min_samples_split -- минимальное число объектов в листе, чтобы сделать новый сплит\n",
    "        \"\"\"\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.num_class = -1\n",
    "        # Для последнего задания\n",
    "        self.feature_importances_ = None\n",
    "        self.criterion = criterion\n",
    "        # Структура, которая описывает дерево\n",
    "        # Представляет словарь, где для  node_id (айдишник узла дерева) храним\n",
    "        # (тип_узла, айдишник признака сплита, порог сплита) если тип NON_LEAF_TYPE\n",
    "        # (тип_узла, предсказание класса, вероятность класса) если тип LEAF_TYPE\n",
    "        # Подразумевается, что у каждого node_id в дереве слева \n",
    "        # узел с айди 2 * node_id + 1, а справа 2 * node_id + 2\n",
    "        self.tree = dict()\n",
    "\n",
    "    def __div_samples(self, x, y, feature_id, threshold):\n",
    "        \"\"\"\n",
    "        Разделяет объекты на 2 множества\n",
    "        x -- матрица объектов\n",
    "        y -- вектор ответов\n",
    "        feature_id -- айдишник признака, по которому делаем сплит\n",
    "        threshold -- порог, по которому делаем сплит\n",
    "        \"\"\"\n",
    "        left_mask = x[:, feature_id] > threshold\n",
    "        right_mask = ~left_mask\n",
    "        return x[left_mask], x[right_mask], y[left_mask], y[right_mask]\n",
    "  \n",
    "    def impurity(self, p):\n",
    "        if self.criterion == 'gini':\n",
    "            return 1 - np.sum(p ** 2)\n",
    "        elif self.criterion == 'entropy':\n",
    "            return -np.sum(p * np.log(p+0.01)/np.log(2))\n",
    "        else:\n",
    "            return 1 - np.max(p)\n",
    "\n",
    "    def __gini_F(self, l, r):\n",
    "        return 1 -(l / (l + r)) ** 2 - (r / (l + r)) ** 2\n",
    "\n",
    "    def __entropy_F(self, l, r):\n",
    "        def mylog2(x):\n",
    "            x[np.where(x < 1e-37)] = 1\n",
    "            return np.log2(x)   \n",
    "        return -(l / (l + r)) * mylog2(l / (l + r))  - (r / (l + r)) * mylog2(r / (l + r))\n",
    "\n",
    "    def __misclass_F(self, l, r):\n",
    "        return 1 - np.maximum(l / (l + r), r / (l + r))\n",
    "    \n",
    "    def impurity(self, l, ls, r, rs):\n",
    "        if self.criterion == 'entropy':\n",
    "            F = self.__entropy_F\n",
    "        elif self.criterion == 'misclass':\n",
    "            F = self.__misclass_F\n",
    "        else:\n",
    "            F = self.__gini_F\n",
    "        return F(l, r) - l / (l + r) * F(ls, l - ls) - r / (l + r) * F(rs, r - rs)\n",
    "    \n",
    "    def __find_threshold(self, x, y):\n",
    "        \"\"\"\n",
    "        Находим оптимальный признак и порог для сплита\n",
    "        Здесь используемые разные impurity в зависимости от self.criterion\n",
    "        \"\"\"\n",
    "\n",
    "        x = x.T\n",
    "        s_x = np.sort(x, axis = 1)\n",
    "        s_y = y[x.argsort(axis = 1)]\n",
    "        s_y = (s_y == y[0]).astype(int)\n",
    "        _, occurs = np.bincount(s_y[0])\n",
    "        sumsl = np.cumsum(s_y, axis = 1)\n",
    "        sumsr = occurs - sumsl\n",
    "        len_ = s_y.shape[1]\n",
    "        \n",
    "        l = np.array(np.array(range(len_)) + np.zeros(s_y.shape[0]).reshape(-1, 1))\n",
    "        l[:, 0] += 1\n",
    "        r = len_ - l\n",
    "        impurities = self.impurity(l, sumsl, r, sumsr)\n",
    "        feature_id, threshold_idx = np.unravel_index(np.argmax(impurities), impurities.shape)\n",
    "        \n",
    "        return feature_id, s_x[feature_id, threshold_idx], impurities[feature_id, threshold_idx]\n",
    "\n",
    "        \"\"\"\n",
    "        x = np.asarray(x)\n",
    "        y = np.asarray(y)\n",
    "        arg_sort_x = np.sort(x, axis = 0)\n",
    "        arg_sort_y = y[x.argsort(axis = 0)]\n",
    "        R = y.size\n",
    "        n_class, counts = np.unique(y, return_counts = True)\n",
    "        H = self.impurity(counts/R)\n",
    "        \n",
    "        def count_q(y):\n",
    "            max_Q = 0\n",
    "            max_idx = [0, 0]\n",
    "            for i in range (0, R - 1):\n",
    "                n_class_left, counts_left = np.unique(y[0:i+1], return_counts = True)\n",
    "                n_class_right, counts_right = np.unique(y[i+1:y.size], return_counts = True)\n",
    "                Rl = i + 1\n",
    "                Rr = y.size - Rl\n",
    "                Hl = self.impurity(counts_left/Rl)\n",
    "                Hr = self.impurity(counts_right/Rr)\n",
    "                Q = H - (Hl * Rl / R) - (Hr * Rr / R)\n",
    "                if Q > max_Q:\n",
    "                    max_Q = Q \n",
    "                    max_idx[0] = Q\n",
    "                    max_idx[1] = i\n",
    "            return max_idx\n",
    "        \n",
    "        result = np.apply_along_axis(count_q, 0, arg_sort_y)\n",
    "        result = result.T\n",
    "        idx = int(np.argmax(result[0]))\n",
    "        n_threshold = int(result[1][idx])\n",
    "        res = np.mean(arg_sort_x[n_threshold:n_threshold + 2, idx])\n",
    "        return idx, res\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        x = np.asarray(x)\n",
    "        y = np.asarray(y)\n",
    "        R = y.size\n",
    "        n_class, counts = np.unique(y, return_counts = True)\n",
    "        H = self.impurity(counts/R)\n",
    "        max_Q = 0\n",
    "        max_idx = [0, 0]\n",
    "        for i in range (x.shape[1]):\n",
    "            feature_response = np.hstack((x[:, i].reshape(-1, 1), y.reshape(-1, 1)))\n",
    "            feature_response = feature_response[feature_response[:, 0].argsort()]\n",
    "            Rl = 0\n",
    "            Rr = R\n",
    "            counts_left = np.zeros(counts.size)\n",
    "            counts_right = np.copy(counts) \n",
    "            for j in range (0, R - 1):\n",
    "                Rl += 1\n",
    "                Rr -= 1\n",
    "                idx = feature_response[j, 1]\n",
    "                counts_left[int(idx) == n_class] += 1\n",
    "                counts_right[int(idx) == n_class] -= 1\n",
    "                Hl = self.impurity(counts_left/Rl)\n",
    "                Hr = self.impurity(counts_right/Rr)\n",
    "                Q = H - (Hl * Rl / R) - (Hr * Rr / R)\n",
    "                if Q > max_Q:\n",
    "                    max_Q = Q \n",
    "                    max_idx[0] = i\n",
    "                    max_idx[1] = np.mean(feature_response[j:j+2, 0])\n",
    "        return max_idx[0],  max_idx[1], max_Q\n",
    "        \"\"\"\n",
    "    def __fit_node(self, x, y, node_id, depth):\n",
    "        \"\"\"\n",
    "        Делаем новый узел в дереве\n",
    "        Решаем, терминальный он или нет\n",
    "        Если нет, то строим левый узел  с айди 2 * node_id + 1\n",
    "        И правый узел с  айди 2 * node_id + 2\n",
    "        \"\"\"\n",
    "        n_class, counts = np.unique(y, return_counts=True)\n",
    "        if (depth == self.max_depth) or (y.size < self.min_samples_split) or (n_class.size == 1):\n",
    "            p = counts/y.size\n",
    "            self.tree[node_id] = [self.LEAF_TYPE, n_class[counts.argmax()], p]\n",
    "        else:\n",
    "            feature_id, threshold, crit_val = self.__find_threshold(x, y)\n",
    "            x_left, x_right, y_left, y_right = self.__div_samples(x, y, feature_id, threshold)\n",
    "            if (y_left.size == 0) or (y_right.size == 0):\n",
    "                p = counts/y.size\n",
    "                self.tree[node_id] = [self.LEAF_TYPE, n_class[counts.argmax()], p]\n",
    "            else:\n",
    "                self.tree[node_id] = [self.NON_LEAF_TYPE, feature_id, threshold]\n",
    "                self.feature_importances_[feature_id] += crit_val\n",
    "                self.__fit_node(x_left, y_left, 2 * node_id + 1, depth + 1)\n",
    "                self.__fit_node(x_right, y_right, 2 * node_id + 2, depth + 1)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        Рекурсивно строим дерево решений\n",
    "        Начинаем с корня node_id 0\n",
    "        \"\"\"\n",
    "        self.num_class = np.unique(y).size\n",
    "        self.feature_importances_ = np.zeros(x.shape[1])\n",
    "        self.__fit_node(x, y, 0, 0)\n",
    "\n",
    "    def __predict_class(self, x, node_id):\n",
    "        \"\"\"\n",
    "        Рекурсивно обходим дерево по всем узлам,\n",
    "        пока не дойдем до терминального\n",
    "        \"\"\"\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_class(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_class(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[1]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Вызывает predict для всех объектов из матрицы X\n",
    "        \"\"\"\n",
    "        return np.array([self.__predict_class(x, 0) for x in X])\n",
    "    \n",
    "    def fit_predict(self, x_train, y_train, predicted_x):\n",
    "        self.fit(x_train, y_train)\n",
    "        return self.predict(predicted_x)\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Возвращает важность признаков\n",
    "        \"\"\"\n",
    "        return self.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n",
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "my_clf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_pred=clf.predict(X_test), y_true=y_test))\n",
    "print(accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ускоряем дерево решений (2 балла)\n",
    "Добиться скорости работы на fit не медленнее чем в 10 раз sklearn на данных wine. \n",
    "Для этого используем numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.42 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.1 ms\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Боевое применение (3 балла)\n",
    "\n",
    "На практике Вы познакомились с датасетом Speed Dating Data. В нем каждая пара в быстрых свиданиях характеризуется определенным набором признаков. Задача -- предсказать, произойдет ли матч пары (колонка match). \n",
    "\n",
    "Пример работы с датасетом можете найти в практике пункт 2\n",
    "https://github.com/VVVikulin/ml1.sphere/blob/master/2019-09/lecture_06/pract-trees.ipynb\n",
    "\n",
    "Данные и описания колонок лежат тут\n",
    "https://cloud.mail.ru/public/8nHV/p6J7wY1y1/speed-dating-experiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте датасет, обработайте данные, как показано на семинаре или своим собственным способом. Обучите дерево классифкации. В качестве таргета возьмите колонку 'match'. Постарайтесь хорошо обработать признаки, чтобы выбить максимальную точность. Если точность будет близка к случайному гаданию, задание не будет защитано. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4099,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Speed Dating Data.csv', encoding='latin1')\n",
    "np.where(pd.isnull(df['income']))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    1,    2, ..., 8243, 8244, 8245], dtype=int64),\n",
       " array([44, 44, 44, ..., 86, 85,  1], dtype=int64))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(axis='columns', thresh=7000)\n",
    "df = df.dropna(thresh=84)\n",
    "np.where(pd.isnull(df)) # нет na нет проблем. Всего лишь потеряли 15 процентов данных\n",
    "# и 100+, наверное, не нужных признак"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>match</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>age_o</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>...</th>\n",
       "      <th>fun3_1</th>\n",
       "      <th>intel3_1</th>\n",
       "      <th>amb3_1</th>\n",
       "      <th>attr</th>\n",
       "      <th>sinc</th>\n",
       "      <th>intel</th>\n",
       "      <th>fun</th>\n",
       "      <th>like</th>\n",
       "      <th>prob</th>\n",
       "      <th>met</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8371</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8376</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6913 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  match  int_corr  samerace  age_o  attr_o  sinc_o  intel_o  \\\n",
       "30         0      0     -0.18         1   27.0     6.0     7.0      8.0   \n",
       "31         0      0     -0.18         1   22.0     6.0     5.0     10.0   \n",
       "32         0      0      0.05         0   22.0    10.0    10.0     10.0   \n",
       "33         0      1     -0.18         1   23.0     7.0     7.0      7.0   \n",
       "34         0      0      0.21         0   24.0     8.0     8.0      9.0   \n",
       "...      ...    ...       ...       ...    ...     ...     ...      ...   \n",
       "8371       1      1      0.59         0   25.0     8.0     6.0      7.0   \n",
       "8372       1      0      0.28         1   24.0     8.0     8.0      7.0   \n",
       "8373       1      0      0.64         0   26.0    10.0     5.0      3.0   \n",
       "8374       1      0      0.71         0   24.0     6.0     3.0      7.0   \n",
       "8376       1      0      0.62         0   22.0     5.0     7.0      5.0   \n",
       "\n",
       "      fun_o  like_o  ...  fun3_1  intel3_1  amb3_1  attr  sinc  intel   fun  \\\n",
       "30      7.0     6.0  ...     9.0       7.0     8.0   4.0  10.0    8.0   5.0   \n",
       "31      6.0     6.0  ...     9.0       7.0     8.0   8.0   7.0    8.0  10.0   \n",
       "32     10.0    10.0  ...     9.0       7.0     8.0   4.0   7.0    8.0   8.0   \n",
       "33      9.0     8.0  ...     9.0       7.0     8.0   8.0  10.0    7.0  10.0   \n",
       "34      8.0     9.0  ...     9.0       7.0     8.0   6.0   9.0    8.0   9.0   \n",
       "...     ...     ...  ...     ...       ...     ...   ...   ...    ...   ...   \n",
       "8371    7.0     5.0  ...     6.0       7.0     7.0   8.0   7.0    8.0   8.0   \n",
       "8372    7.0     7.0  ...     6.0       7.0     7.0   7.0   5.0    5.0   5.0   \n",
       "8373    2.0     6.0  ...     6.0       7.0     7.0   3.0   5.0    5.0   5.0   \n",
       "8374    3.0     2.0  ...     6.0       7.0     7.0   4.0   6.0    8.0   4.0   \n",
       "8376    5.0     6.0  ...     6.0       7.0     7.0   4.0   6.0    5.0   4.0   \n",
       "\n",
       "      like  prob  met  \n",
       "30     6.0   7.0  2.0  \n",
       "31     8.0   1.0  1.0  \n",
       "32     4.0   1.0  2.0  \n",
       "33     8.0  10.0  1.0  \n",
       "34     7.0   7.0  2.0  \n",
       "...    ...   ...  ...  \n",
       "8371   7.0   6.0  0.0  \n",
       "8372   4.0   4.0  0.0  \n",
       "8373   2.0   5.0  0.0  \n",
       "8374   4.0   4.0  0.0  \n",
       "8376   5.0   5.0  0.0  \n",
       "\n",
       "[6913 rows x 43 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['id'], axis = 1)\n",
    "df = df.drop(['dec'], axis = 1)\n",
    "df = df.drop(['dec_o'], axis = 1)\n",
    "df = df.drop(['iid'], axis = 1)\n",
    "df = df.drop(['idg'], axis = 1)\n",
    "df = df.drop(['condtn'], axis = 1)\n",
    "df = df.drop(['from'], axis = 1)\n",
    "df = df.drop(['round'], axis = 1)\n",
    "df = df.drop(['wave'], axis = 1)\n",
    "df = df.drop(['position'], axis = 1)\n",
    "df = df.drop(['order'], axis = 1)\n",
    "df = df.drop(['partner'], axis = 1)\n",
    "df = df.drop(['pid'], axis = 1)\n",
    "df = df.drop(['field'], axis = 1)\n",
    "df = df.drop(['race_o','race'], axis = 1)\n",
    "df = df.drop(['career'], axis = 1)\n",
    "df = df.drop(['pf_o_att', \n",
    "              'pf_o_sin', 'pf_o_int',\n",
    "              'pf_o_fun', 'pf_o_amb', 'pf_o_sha'], axis = 1)\n",
    "df = df.drop(['sports', 'tvsports', 'dining','museums', 'exercise',\n",
    "              'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv',\n",
    "              'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga'], axis = 1)\n",
    "df = df.drop(['exphappy'], axis = 1)\n",
    "#df = df.drop(['undergra'], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.  , -0.18,  1.  , ...,  6.  ,  7.  ,  2.  ],\n",
       "        [ 0.  , -0.18,  1.  , ...,  8.  ,  1.  ,  1.  ],\n",
       "        [ 0.  ,  0.05,  0.  , ...,  4.  ,  1.  ,  2.  ],\n",
       "        ...,\n",
       "        [ 1.  ,  0.64,  0.  , ...,  2.  ,  5.  ,  0.  ],\n",
       "        [ 1.  ,  0.71,  0.  , ...,  4.  ,  4.  ,  0.  ],\n",
       "        [ 1.  ,  0.62,  0.  , ...,  5.  ,  5.  ,  0.  ]]),\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int64))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(df.drop(['match'], axis = 1))\n",
    "y = np.array(df['match'])\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>match</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>age_o</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>...</th>\n",
       "      <th>fun3_1</th>\n",
       "      <th>intel3_1</th>\n",
       "      <th>amb3_1</th>\n",
       "      <th>attr</th>\n",
       "      <th>sinc</th>\n",
       "      <th>intel</th>\n",
       "      <th>fun</th>\n",
       "      <th>like</th>\n",
       "      <th>prob</th>\n",
       "      <th>met</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8371</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8376</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6913 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  match  int_corr  samerace  age_o  attr_o  sinc_o  intel_o  \\\n",
       "30         0      0     -0.18         1   27.0     6.0     7.0      8.0   \n",
       "31         0      0     -0.18         1   22.0     6.0     5.0     10.0   \n",
       "32         0      0      0.05         0   22.0    10.0    10.0     10.0   \n",
       "33         0      1     -0.18         1   23.0     7.0     7.0      7.0   \n",
       "34         0      0      0.21         0   24.0     8.0     8.0      9.0   \n",
       "...      ...    ...       ...       ...    ...     ...     ...      ...   \n",
       "8371       1      1      0.59         0   25.0     8.0     6.0      7.0   \n",
       "8372       1      0      0.28         1   24.0     8.0     8.0      7.0   \n",
       "8373       1      0      0.64         0   26.0    10.0     5.0      3.0   \n",
       "8374       1      0      0.71         0   24.0     6.0     3.0      7.0   \n",
       "8376       1      0      0.62         0   22.0     5.0     7.0      5.0   \n",
       "\n",
       "      fun_o  like_o  ...  fun3_1  intel3_1  amb3_1  attr  sinc  intel   fun  \\\n",
       "30      7.0     6.0  ...     9.0       7.0     8.0   4.0  10.0    8.0   5.0   \n",
       "31      6.0     6.0  ...     9.0       7.0     8.0   8.0   7.0    8.0  10.0   \n",
       "32     10.0    10.0  ...     9.0       7.0     8.0   4.0   7.0    8.0   8.0   \n",
       "33      9.0     8.0  ...     9.0       7.0     8.0   8.0  10.0    7.0  10.0   \n",
       "34      8.0     9.0  ...     9.0       7.0     8.0   6.0   9.0    8.0   9.0   \n",
       "...     ...     ...  ...     ...       ...     ...   ...   ...    ...   ...   \n",
       "8371    7.0     5.0  ...     6.0       7.0     7.0   8.0   7.0    8.0   8.0   \n",
       "8372    7.0     7.0  ...     6.0       7.0     7.0   7.0   5.0    5.0   5.0   \n",
       "8373    2.0     6.0  ...     6.0       7.0     7.0   3.0   5.0    5.0   5.0   \n",
       "8374    3.0     2.0  ...     6.0       7.0     7.0   4.0   6.0    8.0   4.0   \n",
       "8376    5.0     6.0  ...     6.0       7.0     7.0   4.0   6.0    5.0   4.0   \n",
       "\n",
       "      like  prob  met  \n",
       "30     6.0   7.0  2.0  \n",
       "31     8.0   1.0  1.0  \n",
       "32     4.0   1.0  2.0  \n",
       "33     8.0  10.0  1.0  \n",
       "34     7.0   7.0  2.0  \n",
       "...    ...   ...  ...  \n",
       "8371   7.0   6.0  0.0  \n",
       "8372   4.0   4.0  0.0  \n",
       "8373   2.0   5.0  0.0  \n",
       "8374   4.0   4.0  0.0  \n",
       "8376   5.0   5.0  0.0  \n",
       "\n",
       "[6913 rows x 43 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте датасет на трейн и валидацию. Подберите на валидации оптимальный критерий  информативности. \n",
    "Постройте графики зависимости точности на валидации от глубины дерева, от минимального числа объектов для сплита. \n",
    "Какой максимальной точности удалось достигнуть?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_samples_split = 1, max_depth =  2, criterion =  gini\n",
      "f1_score = 0.8280063092512252\n",
      "\n",
      "min_samples_split = 5, max_depth =  2, criterion =  gini\n",
      "f1_score = 0.8261249573190327\n",
      "\n",
      "min_samples_split = 10, max_depth =  2, criterion =  gini\n",
      "f1_score = 0.8300303913593637\n",
      "\n",
      "min_samples_split = 15, max_depth =  2, criterion =  gini\n",
      "f1_score = 0.8307527667309391\n",
      "\n",
      "min_samples_split = 20, max_depth =  2, criterion =  gini\n",
      "f1_score = 0.8274243417088455\n",
      "\n",
      "min_samples_split = 1, max_depth =  4, criterion =  gini\n",
      "f1_score = 0.8391445980959267\n",
      "\n",
      "min_samples_split = 5, max_depth =  4, criterion =  gini\n",
      "f1_score = 0.8350924796135616\n",
      "\n",
      "min_samples_split = 10, max_depth =  4, criterion =  gini\n",
      "f1_score = 0.8353827729573391\n",
      "\n",
      "min_samples_split = 15, max_depth =  4, criterion =  gini\n",
      "f1_score = 0.8391438449023861\n",
      "\n",
      "min_samples_split = 20, max_depth =  4, criterion =  gini\n",
      "f1_score = 0.835382898489596\n",
      "\n",
      "min_samples_split = 1, max_depth =  5, criterion =  gini\n",
      "f1_score = 0.8389986040813048\n",
      "\n",
      "min_samples_split = 5, max_depth =  5, criterion =  gini\n",
      "f1_score = 0.8436296145657588\n",
      "\n",
      "min_samples_split = 10, max_depth =  5, criterion =  gini\n",
      "f1_score = 0.8430489651120752\n",
      "\n",
      "min_samples_split = 15, max_depth =  5, criterion =  gini\n",
      "f1_score = 0.8404454261066924\n",
      "\n",
      "min_samples_split = 20, max_depth =  5, criterion =  gini\n",
      "f1_score = 0.8387083735036555\n",
      "\n",
      "min_samples_split = 1, max_depth =  8, criterion =  gini\n",
      "f1_score = 0.8306078397404998\n",
      "\n",
      "min_samples_split = 5, max_depth =  8, criterion =  gini\n",
      "f1_score = 0.8319128730818671\n",
      "\n",
      "min_samples_split = 10, max_depth =  8, criterion =  gini\n",
      "f1_score = 0.8220734664979513\n",
      "\n",
      "min_samples_split = 15, max_depth =  8, criterion =  gini\n",
      "f1_score = 0.8359596564433197\n",
      "\n",
      "min_samples_split = 20, max_depth =  8, criterion =  gini\n",
      "f1_score = 0.8346610252470473\n",
      "\n",
      "min_samples_split = 1, max_depth =  10, criterion =  gini\n",
      "f1_score = 0.8161440758817387\n",
      "\n",
      "min_samples_split = 5, max_depth =  10, criterion =  gini\n",
      "f1_score = 0.8155636774925684\n",
      "\n",
      "min_samples_split = 10, max_depth =  10, criterion =  gini\n",
      "f1_score = 0.8264150623644252\n",
      "\n",
      "min_samples_split = 15, max_depth =  10, criterion =  gini\n",
      "f1_score = 0.8191810777697438\n",
      "\n",
      "min_samples_split = 20, max_depth =  10, criterion =  gini\n",
      "f1_score = 0.8293071372619908\n",
      "\n",
      "min_samples_split = 1, max_depth =  15, criterion =  gini\n",
      "f1_score = 0.8042849180525428\n",
      "\n",
      "min_samples_split = 5, max_depth =  15, criterion =  gini\n",
      "f1_score = 0.8226525467984253\n",
      "\n",
      "min_samples_split = 10, max_depth =  15, criterion =  gini\n",
      "f1_score = 0.8171553637422672\n",
      "\n",
      "min_samples_split = 15, max_depth =  15, criterion =  gini\n",
      "f1_score = 0.8129605778500842\n",
      "\n",
      "min_samples_split = 20, max_depth =  15, criterion =  gini\n",
      "f1_score = 0.8212066662649634\n",
      "\n",
      "min_samples_split = 1, max_depth =  2, criterion =  entropy\n",
      "f1_score = 0.829450557865349\n",
      "\n",
      "min_samples_split = 5, max_depth =  2, criterion =  entropy\n",
      "f1_score = 0.8301760087772155\n",
      "\n",
      "min_samples_split = 10, max_depth =  2, criterion =  entropy\n",
      "f1_score = 0.8246795161484695\n",
      "\n",
      "min_samples_split = 15, max_depth =  2, criterion =  entropy\n",
      "f1_score = 0.8324886267775368\n",
      "\n",
      "min_samples_split = 20, max_depth =  2, criterion =  entropy\n",
      "f1_score = 0.8264121751225195\n",
      "\n",
      "min_samples_split = 1, max_depth =  4, criterion =  entropy\n",
      "f1_score = 0.8361065919498675\n",
      "\n",
      "min_samples_split = 5, max_depth =  4, criterion =  entropy\n",
      "f1_score = 0.8343692255161885\n",
      "\n",
      "min_samples_split = 10, max_depth =  4, criterion =  entropy\n",
      "f1_score = 0.8408793283522132\n",
      "\n",
      "min_samples_split = 15, max_depth =  4, criterion =  entropy\n",
      "f1_score = 0.8398668479352454\n",
      "\n",
      "min_samples_split = 20, max_depth =  4, criterion =  entropy\n",
      "f1_score = 0.8345140269743713\n",
      "\n",
      "min_samples_split = 1, max_depth =  5, criterion =  entropy\n",
      "f1_score = 0.8460872223226481\n",
      "\n",
      "min_samples_split = 5, max_depth =  5, criterion =  entropy\n",
      "f1_score = 0.8414596639752551\n",
      "\n",
      "min_samples_split = 10, max_depth =  5, criterion =  entropy\n",
      "f1_score = 0.8413146114525588\n",
      "\n",
      "min_samples_split = 15, max_depth =  5, criterion =  entropy\n",
      "f1_score = 0.8436274805173939\n",
      "\n",
      "min_samples_split = 20, max_depth =  5, criterion =  entropy\n",
      "f1_score = 0.8404463048324896\n",
      "\n",
      "min_samples_split = 1, max_depth =  8, criterion =  entropy\n",
      "f1_score = 0.8288724190568009\n",
      "\n",
      "min_samples_split = 5, max_depth =  8, criterion =  entropy\n",
      "f1_score = 0.8269932639391019\n",
      "\n",
      "min_samples_split = 10, max_depth =  8, criterion =  entropy\n",
      "f1_score = 0.8275702729573391\n",
      "\n",
      "min_samples_split = 15, max_depth =  8, criterion =  entropy\n",
      "f1_score = 0.832778355226159\n",
      "\n",
      "min_samples_split = 20, max_depth =  8, criterion =  entropy\n",
      "f1_score = 0.8274267268217241\n",
      "\n",
      "min_samples_split = 1, max_depth =  10, criterion =  entropy\n",
      "f1_score = 0.8193269462521089\n",
      "\n",
      "min_samples_split = 5, max_depth =  10, criterion =  entropy\n",
      "f1_score = 0.8262695704788302\n",
      "\n",
      "min_samples_split = 10, max_depth =  10, criterion =  entropy\n",
      "f1_score = 0.8158544101992448\n",
      "\n",
      "min_samples_split = 15, max_depth =  10, criterion =  entropy\n",
      "f1_score = 0.8204846047240298\n",
      "\n",
      "min_samples_split = 20, max_depth =  10, criterion =  entropy\n",
      "f1_score = 0.8183125828512895\n",
      "\n",
      "min_samples_split = 1, max_depth =  15, criterion =  entropy\n",
      "f1_score = 0.8148415531855066\n",
      "\n",
      "min_samples_split = 5, max_depth =  15, criterion =  entropy\n",
      "f1_score = 0.8209177537760102\n",
      "\n",
      "min_samples_split = 10, max_depth =  15, criterion =  entropy\n",
      "f1_score = 0.8141196171768298\n",
      "\n",
      "min_samples_split = 15, max_depth =  15, criterion =  entropy\n",
      "f1_score = 0.823665466578292\n",
      "\n",
      "min_samples_split = 20, max_depth =  15, criterion =  entropy\n",
      "f1_score = 0.8327789201213144\n",
      "\n",
      "min_samples_split = 1, max_depth =  2, criterion =  misclass\n",
      "f1_score = 0.8246777586968749\n",
      "\n",
      "min_samples_split = 5, max_depth =  2, criterion =  misclass\n",
      "f1_score = 0.8246781352936451\n",
      "\n",
      "min_samples_split = 10, max_depth =  2, criterion =  misclass\n",
      "f1_score = 0.8246778214630032\n",
      "\n",
      "min_samples_split = 15, max_depth =  2, criterion =  misclass\n",
      "f1_score = 0.824678449124287\n",
      "\n",
      "min_samples_split = 20, max_depth =  2, criterion =  misclass\n",
      "f1_score = 0.8246787001888004\n",
      "\n",
      "min_samples_split = 1, max_depth =  4, criterion =  misclass\n",
      "f1_score = 0.8271390068892103\n",
      "\n",
      "min_samples_split = 5, max_depth =  4, criterion =  misclass\n",
      "f1_score = 0.8304634776452158\n",
      "\n",
      "min_samples_split = 10, max_depth =  4, criterion =  misclass\n",
      "f1_score = 0.8246770055033341\n",
      "\n",
      "min_samples_split = 15, max_depth =  4, criterion =  misclass\n",
      "f1_score = 0.8284392700048205\n",
      "\n",
      "min_samples_split = 20, max_depth =  4, criterion =  misclass\n",
      "f1_score = 0.8246768172049489\n",
      "\n",
      "min_samples_split = 1, max_depth =  5, criterion =  misclass\n",
      "f1_score = 0.8252566506989636\n",
      "\n",
      "min_samples_split = 5, max_depth =  5, criterion =  misclass\n",
      "f1_score = 0.8304651095645538\n",
      "\n",
      "min_samples_split = 10, max_depth =  5, criterion =  misclass\n",
      "f1_score = 0.8264134304450872\n",
      "\n",
      "min_samples_split = 15, max_depth =  5, criterion =  misclass\n",
      "f1_score = 0.8246768172049489\n",
      "\n",
      "min_samples_split = 20, max_depth =  5, criterion =  misclass\n",
      "f1_score = 0.8255460653169439\n",
      "\n",
      "min_samples_split = 1, max_depth =  8, criterion =  misclass\n",
      "f1_score = 0.8258376139832891\n",
      "\n",
      "min_samples_split = 5, max_depth =  8, criterion =  misclass\n",
      "f1_score = 0.8235212300152647\n",
      "\n",
      "min_samples_split = 10, max_depth =  8, criterion =  misclass\n",
      "f1_score = 0.8255466929782277\n",
      "\n",
      "min_samples_split = 15, max_depth =  8, criterion =  misclass\n",
      "f1_score = 0.8290196056278623\n",
      "\n",
      "min_samples_split = 20, max_depth =  8, criterion =  misclass\n",
      "f1_score = 0.8282961004659756\n",
      "\n",
      "min_samples_split = 1, max_depth =  10, criterion =  misclass\n",
      "f1_score = 0.828438705109665\n",
      "\n",
      "min_samples_split = 5, max_depth =  10, criterion =  misclass\n",
      "f1_score = 0.8252552698441392\n",
      "\n",
      "min_samples_split = 10, max_depth =  10, criterion =  misclass\n",
      "f1_score = 0.8323467125612597\n",
      "\n",
      "min_samples_split = 15, max_depth =  10, criterion =  misclass\n",
      "f1_score = 0.8269925107455611\n",
      "\n",
      "min_samples_split = 20, max_depth =  10, criterion =  misclass\n",
      "f1_score = 0.8251111588133687\n",
      "\n",
      "min_samples_split = 1, max_depth =  15, criterion =  misclass\n",
      "f1_score = 0.8277150744155218\n",
      "\n",
      "min_samples_split = 5, max_depth =  15, criterion =  misclass\n",
      "f1_score = 0.8230873277697438\n",
      "\n",
      "min_samples_split = 10, max_depth =  15, criterion =  misclass\n",
      "f1_score = 0.8262684406885193\n",
      "\n",
      "min_samples_split = 15, max_depth =  15, criterion =  misclass\n",
      "f1_score = 0.8288736743793685\n",
      "\n",
      "min_samples_split = 20, max_depth =  15, criterion =  misclass\n",
      "f1_score = 0.8248218069615169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 3, shuffle = True)\n",
    "\n",
    "min_samples_range = [1, 5, 10, 15, 20]\n",
    "depth_range = [2, 4, 5, 8, 10, 15]\n",
    "crit_range = ['gini', 'entropy', 'misclass']\n",
    "\n",
    "plot_dict = {'gini' : [ [i for i in range(21)] for j in range(16) ],\n",
    "             'entropy' : [ [i for i in range(21)] for j in range(16) ],\n",
    "             'misclass' : [ [i for i in range(21)] for j in range(16) ]}\n",
    "\n",
    "best_model = {}\n",
    "max_score = 0.0\n",
    "\n",
    "for crit in crit_range:\n",
    "    for depth in depth_range:\n",
    "        for min_samples in min_samples_range:\n",
    "            clf = MyDecisionTreeClassifier(min_samples_split = min_samples, max_depth = depth, criterion = crit)\n",
    "            ind = kf.split(X)\n",
    "            scores = []\n",
    "            for train_ind, test_ind in ind:\n",
    "                clf.fit(X[train_ind], y[train_ind])\n",
    "                scores.append(accuracy_score(y[test_ind], clf.predict(X[test_ind])))\n",
    "            score = np.mean(np.array(scores))\n",
    "            \n",
    "            if score > max_score:\n",
    "                best_model = {'min_samples_split' : min_samples, 'max_depth' : depth, 'criterion' : crit}\n",
    "                max_score = score\n",
    "                \n",
    "            print('''min_samples_split = {}, max_depth =  {}, criterion =  {}\\nf1_score = {}\\n'''\\\n",
    "                                        .format(min_samples, depth, crit, score))\n",
    "            \n",
    "            plot_dict[crit][depth][min_samples] = score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 1, 'max_depth': 5, 'criterion': 'entropy'}\n",
      "0.8460872223226481\n"
     ]
    }
   ],
   "source": [
    "print(best_model)\n",
    "print(max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = plot_dict[best_model['criterion']]\n",
    "\n",
    "scores_from_min_sample = [scores[best_model['max_depth']][min_samples] for min_samples in min_samples_range]\n",
    "scores_from_max_depth = [scores[max_depth][best_model['min_samples_split']] for max_depth in depth_range]\n",
    "\n",
    "plt.plot(min_samples_range, scores_from_min_sample)\n",
    "\n",
    "plt.xlim(0, 25)\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(depth_range, scores_from_max_depth)\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Находим самые важные признаки (2 балла)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По построенному дереву  легко понять, какие признаки лучше всего помогли решить задачу. Часто это бывает нужно  не только  для сокращения размерности в данных, но и для лучшего понимания прикладной задачи. Например, Вы хотите понять, какие признаки стоит еще конструировать -- для этого нужно понимать, какие из текущих лучше всего работают в дереве. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой метод -- посчитать число сплитов, где использовался данные признак. Это не лучший вариант, так как по признаку который принимает всего 2 значения, но который почти точно разделяет выборку, число сплитов будет очень 1, но при этом признак сам очень хороший. \n",
    "В этом задании предлагается для каждого признака считать суммарный gain (в лекции обозначено как Q) при использовании этого признака в сплите. Тогда даже у очень хороших признаков с маленьким число сплитов это значение должно быть довольно высоким.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать это довольно просто: создаете словарь номер фичи : суммарный гейн и добавляете в нужную фичу каждый раз, когда используете ее при построении дерева. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавьте функционал, который определяет значения feature importance. Обучите дерево на датасете Speed Dating Data.\n",
    "Выведите 10 главных фичей по важности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "samp = best_model['min_samples_split']\n",
    "depth = best_model['max_depth']\n",
    "crit = best_model['criterion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mytree = MyDecisionTreeClassifier(min_samples_split = samp, max_depth = depth, criterion = crit)\n",
    "mytree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8554913294797688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.20399787,\n",
       "       0.        , 0.        , 0.65925053, 2.21037078, 1.54358167,\n",
       "       0.        , 0.        , 0.        , 0.82694266, 0.35793694,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.0313874 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.25527294, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.84835277, 0.        , 0.        , 0.        ,\n",
       "       0.58316212, 0.        , 0.        , 1.58512074, 2.49679381,\n",
       "       1.76520308, 0.51666368])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp = mytree.get_feature_importance()\n",
    "sorted_indices = np.argsort(-feat_imp)\n",
    "y_pred = mytree.predict(X_test)\n",
    "print(accuracy_score(y_pred, y_test))\n",
    "feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like' 'like_o' 'prob' 'fun' 'prob_o' 'sinc3_1' 'imprelig' 'fun_o' 'attr'\n",
      " 'met']\n"
     ]
    }
   ],
   "source": [
    "final_df = df.drop(['match'], axis = 1)\n",
    "features = final_df.columns.values\n",
    "imp_feat = features[sorted_indices[:10]]\n",
    "print(imp_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения деревьев решений Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ваш ответ здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Здесь Вы можете оставить отзыв о этой домашней работе или о всем курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ВАШ ОТЗЫВ ЗДЕСЬ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
