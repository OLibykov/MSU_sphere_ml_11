{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №4 - Градиентный бустинг\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 13 июня 2022, 08:30   \n",
    "**Штраф за опоздание:** -2 балла после 08:30 13 июня, -4 балла после 08:30 20 июня, -6 баллов после 08:30 24 мая, -8 баллов после 08:30 31 мая.\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла Присылать ДЗ необходимо в виде ссылки на свой github репозиторий на почту ml1.sphere@mail.ru с указанием темы в следующем формате:\n",
    "[ML0422, Задание 4] Фамилия Имя. \n",
    "\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Считаем производные для функций потерь (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем реализовать градиентный бустинг для 3 функций потерь:\n",
    "\n",
    "1) MSE  $L(a(x_i), y_i) = (y_i - a(x_i)) ^ 2$\n",
    "\n",
    "2) Экспоненциальная  $L(a(x_i), y_i) = exp( -a(x_i) y_i), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "3) Логистическая  $L(a(x_i), y_i) = \\log (1 + exp( -a(x_i) y_i)), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "где $a(x_i)$ предсказание бустинга на итом объекте. \n",
    "\n",
    "Для каждой функции потерь напишите таргет, на который будет настраиваться каждое дерево в бустинге. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваше решение тут"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для MSE:\n",
    "\n",
    "$$\n",
    "y_{new}= -\\frac{\\partial L(F, y)}{\\partial F}=-2(F_{k-1}(x_{i}) - y_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для экспоненциального лосса:\n",
    "\n",
    "$$\n",
    "y_{new}= -\\frac{\\partial L(F, y)}{\\partial F}=y_i \\exp(-F_{k-1}(x_i)y_i) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для логлосса:\n",
    "\n",
    "$$\n",
    "y_{new}= -\\frac{\\partial L(F, y)}{\\partial F}=\\frac{y_i \\exp(-F_{k-1}(x_i)y_i)}{1+exp(-F_{k-1}(x_i)y_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Реализуем градиентный бустинг (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс градиентного бустинга для классификации. Ваша реализация бустинга должна работать по точности не более чем на 5 процентов хуже чем GradientBoostingClassifier из sklearn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детали реализации:\n",
    "\n",
    "-- должно поддерживаться 3 функции потерь\n",
    "\n",
    "-- сами базовые алгоритмы(деревья, линейные модели и тп) реализовать не надо, просто возьмите готовые из sklearn\n",
    "\n",
    "-- в качестве функции потерь для построения одного дерева используйте MSE\n",
    "\n",
    "-- шаг в бустинге можно не подбирать, можно брать константный\n",
    "\n",
    "-- можно брать разные модели в качестве инициализации бустинга\n",
    "\n",
    "-- должны поддерживаться следующие параметры:\n",
    "\n",
    "а) число итераций\n",
    "б) размер шага\n",
    "в) процент случайных фичей при построении одного дерева\n",
    "д) процент случайных объектов при построении одного дерева\n",
    "е) параметры базового алгоритма (передавайте через **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyGradientBoostingClassifier:\n",
    "\n",
    "    def __init__(self, loss = \"MSE\",learning_rate = 0.1, n_estimators = 100,\n",
    "                 colsample = 1, subsample = 1, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        loss -- один из 3 лоссов:\n",
    "        learning_rate -- шаг бустинга\n",
    "        n_estimators -- число итераций\n",
    "        colsample -- процент рандомных признаков при обучении одного алгоритма\n",
    "        subsample -- процент рандомных объектов при обучении одного алгоритма\n",
    "        args, kwargs -- параметры  базовых моделей\n",
    "        \"\"\"\n",
    "        # Ваш код здесь\n",
    "        self.loss = loss\n",
    "        self.learning_rate = 0.1\n",
    "        self.n_estimators = n_estimators\n",
    "        self.colsample = colsample\n",
    "        self.subsample = subsample\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        self.list_model = []\n",
    "        self.num_features = []\n",
    "        self.F_x = None\n",
    "        self.start_const = None\n",
    "        self.start_model = None\n",
    "        self.y_bi = False\n",
    "    \n",
    "    def calc_new_y(self, y):\n",
    "        if self.loss == \"MSE\":\n",
    "            return -2*(self.F_x - y)\n",
    "        elif self.loss == \"log_loss\":\n",
    "            return y*np.exp(-self.F_x * y)\n",
    "        else:\n",
    "            return y*np.exp(-self.F_x * y)/(1 + np.exp(-self.F_x * y))\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, base_model = DecisionTreeRegressor, init_model=None):\n",
    "        \"\"\"\n",
    "        X -- объекты для обучения:\n",
    "        y -- таргеты для обучения\n",
    "        base_model -- класс базовых моделей, например sklearn.tree.DecisionTreeRegressor\n",
    "        init_model -- класс для первой модели, если None то берем константу (только для посл задания)\n",
    "        \"\"\"\n",
    "        # Ваш код здесь\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        if (np.unique(y)[0] == -1 and np.unique(y)[1] == 1):\n",
    "            self.y_bi = True\n",
    "        \n",
    "        if init_model == None:\n",
    "            index_str = np.sort(np.random.choice(y.size, int(y.size * self.subsample), replace=False))\n",
    "            self.start_const = np.mean(y[index_str])\n",
    "            #self.F_x = np.full(int(y.size * self.subsample), self.start_const)\n",
    "        else:\n",
    "            clf = init_model()\n",
    "            index_str = np.sort(np.random.choice(y.size, int(y.size * self.subsample), replace=False))\n",
    "            clf.fit(X[index_str], y[index_str])\n",
    "            #self.F_x = clf.predict(X[index_str])\n",
    "            self.start_model = clf\n",
    "            \n",
    "        #self.F_x = np.asarray(self.F_x)\n",
    "        for i in range(self.n_estimators):\n",
    "            index_str = np.sort(np.random.choice(y.size, int(y.size * self.subsample), replace=False))\n",
    "            index_col = np.sort(np.random.choice(X.shape[1], int(X.shape[1] * self.colsample), replace=False))\n",
    "            \n",
    "            if self.start_model == None:\n",
    "                self.F_x = np.full(int(y.size * self.subsample), self.start_const)\n",
    "            else:\n",
    "                self.F_x = self.start_model.predict(X[index_str])\n",
    "            for i, clf in enumerate(self.list_model):\n",
    "                self.F_x += self.learning_rate * clf.predict(X[np.ix_(index_str, self.num_features[i])])\n",
    "            self.F_x = np.asarray(self.F_x)    \n",
    "                \n",
    "            clf = base_model(*self.args, **self.kwargs)\n",
    "            clf.fit(X[np.ix_(index_str, index_col)], self.calc_new_y(y[index_str]))\n",
    "            #self.F_x += self.learning_rate * clf.predict(X[np.ix_(index_str, index_col)])\n",
    "            self.list_model.append(clf)\n",
    "            self.num_features.append(index_col)\n",
    "            \n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Ваш код здесь\n",
    "        X = np.asarray(X)\n",
    "        if self.start_model == None:\n",
    "            pred = np.full(X.shape[0], self.start_const)\n",
    "        else:\n",
    "            pred = self.start_model.predict(X)\n",
    "        \n",
    "        for i, clf in enumerate(self.list_model):\n",
    "            pred += self.learning_rate * clf.predict(X[:,self.num_features[i]])\n",
    "        if not(self.y_bi):\n",
    "            return np.around(pred).astype(int)\n",
    "        else:\n",
    "            pred = np.asarray(pred)\n",
    "            ind = np.abs(pred - 1) >= np.abs(pred + 1)\n",
    "            ind = np.asarray(ind)\n",
    "            pred[ind] = -1\n",
    "            pred[np.logical_not(ind)] = 1\n",
    "            return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyGradientBoostingClassifier()\n",
    "clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n",
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "my_clf.fit(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "my_y_pred=my_clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_pred=clf.predict(X_test), y_true=y_test))\n",
    "print(accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбираем параметры (2 балла)\n",
    "\n",
    "Давайте попробуем применить Ваш бустинг для предсказаний цены домов в Калифорнии. Чтобы можно было попробовтаь разные функции потерь, переведем по порогу таргет в 2 класса: дорогие и дешевые дома."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании нужно\n",
    "\n",
    "1) Построить график точности в зависимости от числа итераций на валидации.\n",
    "\n",
    "2) Подобрать оптимальные параметры Вашего бустинга на валидации. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "X, y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8) (20640,)\n"
     ]
    }
   ],
   "source": [
    "# Превращаем регрессию в классификацию\n",
    "y = (y > 2.0).astype(int)\n",
    "y[y == 0] = -1\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametrs_1 = { 'n_estimators': [5, 10, 20, 30, 50, 100]}\n",
    "parametrs_2 = { 'n_estimators': [10, 20, 30],\n",
    "              'loss': [\"MSE\", \"log_loss\", \"Exp\"],\n",
    "              'learning_rate': [0.1, 0.3, 0.5],\n",
    "              'subsample': [0.5, 0.8, 1.0],\n",
    "              'colsample': [0.5, 0.8, 1.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 5, score = 0.9319282945736435\n",
      "\n",
      "n_estimators = 10, score = 0.923546511627907\n",
      "\n",
      "n_estimators = 20, score = 0.9239341085271319\n",
      "\n",
      "n_estimators = 30, score = 0.9213178294573643\n",
      "\n",
      "n_estimators = 50, score = 0.9227228682170542\n",
      "\n",
      "n_estimators = 100, score = 0.9191375968992248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 3, shuffle = True)\n",
    "res = []\n",
    "for n_est in parametrs_1['n_estimators']:\n",
    "    clf = MyGradientBoostingClassifier(n_estimators=n_est)\n",
    "    ind = kf.split(X)\n",
    "    scores = []\n",
    "    for train_ind, test_ind in ind:\n",
    "        clf.fit(X[train_ind], y[train_ind])\n",
    "        scores.append(accuracy_score(y[test_ind], clf.predict(X[test_ind])))\n",
    "    score = np.mean(np.array(scores))\n",
    "    \n",
    "    print('''n_estimators = {}, score = {}\\n'''.format(n_est, score))\n",
    "    res.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(parametrs_1['n_estimators'], res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом увеличение числа итераций градиентного бустинга не влияет на качество модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 10, loss = MSE, learning_rate = 0.1, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9429748062015504\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.1, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9531976744186047\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.1, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9513565891472867\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.1, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9540697674418605\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.1, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9519864341085271\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.1, subsample = 0.8, colsample = 1.0\n",
      " score = 0.952906976744186\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.1, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9547480620155039\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.1, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9547480620155039\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.1, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9254360465116278\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.3, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9473352713178295\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.3, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9529069767441861\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.3, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9482073643410853\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.3, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9500968992248061\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.3, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9544089147286822\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.3, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9548934108527133\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.3, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9423449612403102\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.3, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9468023255813952\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.3, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9246124031007752\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.5, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9434108527131783\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.5, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9469476744186046\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.5, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9483527131782946\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.5, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9537790697674419\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.5, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9496608527131783\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.5, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9551841085271318\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.5, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9549418604651163\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.5, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9573158914728682\n",
      "\n",
      "n_estimators = 10, loss = MSE, learning_rate = 0.5, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9249515503875969\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.1, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9333817829457365\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.1, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9369670542635659\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.1, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9386627906976744\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.1, subsample = 0.8, colsample = 0.5\n",
      " score = 0.946172480620155\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.1, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9604166666666667\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.1, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9522771317829456\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.1, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9492732558139535\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.1, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9408914728682171\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.1, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9400193798449612\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.3, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9296996124031008\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.3, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9494186046511629\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.3, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9381782945736434\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.3, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9401162790697675\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.3, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9555232558139535\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.3, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9524709302325581\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.3, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9538759689922481\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.3, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9542151162790699\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.3, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9378391472868217\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.5, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9432170542635658\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.5, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9404554263565892\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.5, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9406976744186046\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.5, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9547480620155039\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.5, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9545542635658913\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.5, subsample = 0.8, colsample = 1.0\n",
      " score = 0.95140503875969\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.5, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9575096899224805\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.5, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9534399224806202\n",
      "\n",
      "n_estimators = 10, loss = log_loss, learning_rate = 0.5, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9416666666666668\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.1, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9288759689922482\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.1, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9314437984496124\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.1, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9199612403100774\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.1, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9397771317829458\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.1, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9460271317829457\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.1, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9449612403100774\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.1, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9405523255813953\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.1, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9506782945736435\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.1, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9176841085271318\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.3, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9243701550387597\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.3, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9303779069767443\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.3, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9236918604651163\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.3, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9320251937984496\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.3, subsample = 0.8, colsample = 0.8\n",
      " score = 0.946220930232558\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.3, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9444282945736434\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.3, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9391472868217053\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.3, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9540213178294573\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 10, loss = Exp, learning_rate = 0.3, subsample = 1.0, colsample = 1.0\n",
      " score = 0.920203488372093\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.5, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9105620155038759\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.5, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9292635658914729\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.5, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9209302325581395\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.5, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9367248062015504\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.5, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9437984496124031\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.5, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9411821705426356\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.5, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9314922480620155\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.5, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9489341085271318\n",
      "\n",
      "n_estimators = 10, loss = Exp, learning_rate = 0.5, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9323158914728683\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.1, subsample = 0.5, colsample = 0.5\n",
      " score = 0.944186046511628\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.1, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9502422480620155\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.1, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9546027131782946\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.1, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9603197674418604\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.1, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9641957364341085\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.1, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9577034883720931\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.1, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9543604651162791\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.1, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9588662790697674\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.1, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9197189922480621\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.3, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9557170542635659\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.3, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9542635658914729\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.3, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9552325581395348\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.3, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9518895348837209\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.3, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9602713178294574\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.3, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9563468992248062\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.3, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9576550387596899\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.3, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9584786821705427\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.3, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9259689922480621\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.5, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9532461240310077\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.5, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9546996124031008\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.5, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9541666666666666\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.5, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9472383720930232\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.5, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9563468992248062\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.5, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9578972868217054\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.5, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9557655038759689\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.5, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9569282945736434\n",
      "\n",
      "n_estimators = 20, loss = MSE, learning_rate = 0.5, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9255329457364341\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.1, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9499031007751938\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.1, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9540213178294574\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.1, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9507751937984495\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.1, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9539244186046512\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.1, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9617732558139535\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.1, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9557655038759689\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.1, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9584786821705427\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.1, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9592054263565891\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.1, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9417635658914728\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.3, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9560077519379845\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.3, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9544573643410853\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.3, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9522771317829458\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.3, subsample = 0.8, colsample = 0.5\n",
      " score = 0.959641472868217\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.3, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9595445736434108\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.3, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9554748062015505\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.3, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9572189922480621\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.3, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9622577519379845\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.3, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9416182170542635\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.5, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9493701550387597\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.5, subsample = 0.5, colsample = 0.8\n",
      " score = 0.951453488372093\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.5, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9499515503875969\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.5, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9515988372093024\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.5, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9616763565891473\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.5, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9575096899224805\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.5, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9555717054263565\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.5, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9585271317829457\n",
      "\n",
      "n_estimators = 20, loss = log_loss, learning_rate = 0.5, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9428779069767442\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.1, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9458817829457363\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.1, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9493701550387597\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.1, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9392441860465116\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.1, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9508236434108527\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.1, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9599321705426357\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.1, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9535852713178294\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.1, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9629844961240309\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 20, loss = Exp, learning_rate = 0.1, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9586724806201551\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.1, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9357073643410853\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.3, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9486434108527132\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.3, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9441375968992247\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.3, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9396317829457365\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.3, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9550387596899225\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.3, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9550387596899225\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.3, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9546027131782946\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.3, subsample = 1.0, colsample = 0.5\n",
      " score = 0.957170542635659\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.3, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9588178294573644\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.3, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9374031007751938\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.5, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9486918604651162\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.5, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9459302325581396\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.5, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9380813953488372\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.5, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9605135658914729\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.5, subsample = 0.8, colsample = 0.8\n",
      " score = 0.959593023255814\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.5, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9543120155038759\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.5, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9539728682170542\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.5, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9527131782945736\n",
      "\n",
      "n_estimators = 20, loss = Exp, learning_rate = 0.5, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9345930232558141\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.1, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9617732558139535\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.1, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9608527131782946\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.1, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9593992248062015\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.1, subsample = 0.8, colsample = 0.5\n",
      " score = 0.957170542635659\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.1, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9624031007751938\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.1, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9570251937984496\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.1, subsample = 1.0, colsample = 0.5\n",
      " score = 0.942781007751938\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.1, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9598837209302326\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.1, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9218023255813953\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.3, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9435077519379845\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.3, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9574127906976745\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.3, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9569767441860465\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.3, subsample = 0.8, colsample = 0.5\n",
      " score = 0.953343023255814\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.3, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9568313953488371\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.3, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9577034883720931\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.3, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9482073643410853\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.3, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9611918604651163\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.3, subsample = 1.0, colsample = 1.0\n",
      " score = 0.921656976744186\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.5, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9452034883720931\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.5, subsample = 0.5, colsample = 0.8\n",
      " score = 0.958187984496124\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.5, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9543604651162791\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.5, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9501453488372094\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.5, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9578488372093023\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.5, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9562499999999999\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.5, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9602713178294574\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.5, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9612887596899226\n",
      "\n",
      "n_estimators = 30, loss = MSE, learning_rate = 0.5, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9200581395348837\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.1, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9551356589147287\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.1, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9563953488372093\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.1, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9554748062015505\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.1, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9598352713178295\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.1, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9624031007751938\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.1, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9587693798449614\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.1, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9609011627906977\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.1, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9616279069767443\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.1, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9429748062015504\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.3, subsample = 0.5, colsample = 0.5\n",
      " score = 0.953391472868217\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.3, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9599321705426357\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.3, subsample = 0.5, colsample = 1.0\n",
      " score = 0.956734496124031\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.3, subsample = 0.8, colsample = 0.5\n",
      " score = 0.959593023255814\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.3, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9613372093023256\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.3, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9593023255813954\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.3, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9593023255813954\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.3, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9596899224806202\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.3, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9419089147286822\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.5, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9624515503875969\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.5, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9617732558139535\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.5, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9546511627906976\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.5, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9621608527131783\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.5, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9632267441860466\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.5, subsample = 0.8, colsample = 1.0\n",
      " score = 0.960513565891473\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 30, loss = log_loss, learning_rate = 0.5, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9623546511627907\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.5, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9610465116279069\n",
      "\n",
      "n_estimators = 30, loss = log_loss, learning_rate = 0.5, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9411337209302326\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.1, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9551356589147287\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.1, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9500484496124031\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.1, subsample = 0.5, colsample = 1.0\n",
      " score = 0.9483042635658915\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.1, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9579941860465117\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.1, subsample = 0.8, colsample = 0.8\n",
      " score = 0.960029069767442\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.1, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9560562015503876\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.1, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9577519379844962\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.1, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9597383720930233\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.1, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9359980620155038\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.3, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9523255813953488\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.3, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9518895348837209\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.3, subsample = 0.5, colsample = 1.0\n",
      " score = 0.944234496124031\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.3, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9601744186046511\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.3, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9625968992248062\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.3, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9578972868217054\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.3, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9570251937984496\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.3, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9614825581395349\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.3, subsample = 1.0, colsample = 1.0\n",
      " score = 0.933672480620155\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.5, subsample = 0.5, colsample = 0.5\n",
      " score = 0.9523255813953488\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.5, subsample = 0.5, colsample = 0.8\n",
      " score = 0.9555717054263565\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.5, subsample = 0.5, colsample = 1.0\n",
      " score = 0.946124031007752\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.5, subsample = 0.8, colsample = 0.5\n",
      " score = 0.9583817829457364\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.5, subsample = 0.8, colsample = 0.8\n",
      " score = 0.9614341085271318\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.5, subsample = 0.8, colsample = 1.0\n",
      " score = 0.9583817829457364\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.5, subsample = 1.0, colsample = 0.5\n",
      " score = 0.9584302325581394\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.5, subsample = 1.0, colsample = 0.8\n",
      " score = 0.9583333333333334\n",
      "\n",
      "n_estimators = 30, loss = Exp, learning_rate = 0.5, subsample = 1.0, colsample = 1.0\n",
      " score = 0.9368217054263566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resv = []\n",
    "best_model = {}\n",
    "max_score = 0.0\n",
    "for n_est in parametrs_2['n_estimators']:\n",
    "    for los in parametrs_2['loss']:\n",
    "        for learn_rate in parametrs_2['learning_rate']:\n",
    "            for subs in parametrs_2['subsample']:\n",
    "                for cols in parametrs_2['colsample']:\n",
    "                    clf = MyGradientBoostingClassifier(n_estimators=n_est, loss = los, \n",
    "                                                       learning_rate = learn_rate, subsample = subs,\n",
    "                                                       colsample = cols)\n",
    "                    ind = kf.split(X)\n",
    "                    scores = []\n",
    "                    for train_ind, test_ind in ind:\n",
    "                        clf.fit(X[train_ind], y[train_ind])\n",
    "                        scores.append(accuracy_score(y[test_ind], clf.predict(X[test_ind])))\n",
    "                    score = np.mean(np.array(scores))\n",
    "                    \n",
    "                    if score > max_score:\n",
    "                        best_model = {'n_estimators' : n_est, 'loss' : los, 'learning_rate' : learn_rate,\n",
    "                                      'subsample' : subs, 'colsample' : cols}\n",
    "                        max_score = score\n",
    "\n",
    "                    print('''n_estimators = {}, loss = {}, learning_rate = {}, subsample = {}, colsample = {}\\n score = {}\\n'''\\\n",
    "                          .format(n_est, los, learn_rate, subs, cols, score))\n",
    "                    resv.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 20, 'loss': 'MSE', 'learning_rate': 0.1, 'subsample': 0.8, 'colsample': 0.8}\n",
      "0.9641957364341085\n"
     ]
    }
   ],
   "source": [
    "print(best_model)\n",
    "print(max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BooBag BagBoo (1 балл)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем объединить бустинг и бэгинг. Давайте\n",
    "\n",
    "1) в качестве базовой модели брать не дерево решений, а случайный лес (из sklearn)\n",
    "\n",
    "2) обучать N бустингов на бустрапированной выборке, а затем предикт усреднять"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте обе этих стратегии на данных из прошлого задания. Получилось ли улучшить качество? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9627906976744186"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MyGradientBoostingClassifier(**best_model)\n",
    "ind = kf.split(X)\n",
    "scores = []\n",
    "for train_ind, test_ind in ind:\n",
    "    clf.fit(X[train_ind], y[train_ind], base_model = RandomForestRegressor)\n",
    "    scores.append(accuracy_score(y[test_ind], clf.predict(X[test_ind])))\n",
    "score = np.mean(np.array(scores))\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество не удалось улучшить. Помимо этого обучение шло безумно долго. Скорее всего алгоритм немного переобучился из-за огромной сложности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_bootstrap_samples(X, y):\n",
    "    indices = np.random.choice(y.size, y.size)\n",
    "    samples = X[indices]\n",
    "    y_new = y[indices]\n",
    "    return samples, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "scors = []\n",
    "for i in range(N):\n",
    "    data, new_y = get_bootstrap_samples(X, y)\n",
    "    clf = MyGradientBoostingClassifier(**best_model)\n",
    "    ind = kf.split(data)\n",
    "    scores = []\n",
    "    for train_ind, test_ind in ind:\n",
    "        clf.fit(data[train_ind], new_y[train_ind])\n",
    "        scores.append(accuracy_score(new_y[test_ind], clf.predict(data[test_ind])))\n",
    "    score = np.mean(np.array(scores))\n",
    "    scors.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9774903100775193"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scors = np.asarray(scors)\n",
    "scors.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Усреднение 5 бустингов дало прирост в качестве примерно на 1.5 процента, но нам потребовалось примерно в 5 раз больше времени, чтобы обучить модель и предсказать результат. Улучшение качества произошло в результате уменьшения дисперсии за счет усреднения результата бустингов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Умная инициализация (1 балл)\n",
    "\n",
    "Попробуйте брать в качестве инициализации бустинга не константу, а какой-то алгоритм и уже от его предикта стартовать итерации бустинга. Попробуйте разные модели из sklearn: линейные модели, рандом форест, svm..\n",
    "\n",
    "Получилось ли улучшить качество? Почему?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9364825581395348"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MyGradientBoostingClassifier(**best_model)\n",
    "ind = kf.split(X)\n",
    "scores = []\n",
    "for train_ind, test_ind in ind:\n",
    "    clf.fit(X[train_ind], y[train_ind], init_model = RandomForestRegressor)\n",
    "    scores.append(accuracy_score(y[test_ind], clf.predict(X[test_ind])))\n",
    "score = np.mean(np.array(scores))\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624031007751938"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MyGradientBoostingClassifier(**best_model)\n",
    "ind = kf.split(X)\n",
    "scores = []\n",
    "for train_ind, test_ind in ind:\n",
    "    clf.fit(X[train_ind], y[train_ind], init_model = LinearRegression)\n",
    "    scores.append(accuracy_score(y[test_ind], clf.predict(X[test_ind])))\n",
    "score = np.mean(np.array(scores))\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9602713178294574"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MyGradientBoostingClassifier(**best_model)\n",
    "ind = kf.split(X)\n",
    "scores = []\n",
    "for train_ind, test_ind in ind:\n",
    "    clf.fit(X[train_ind], y[train_ind], init_model = SVR)\n",
    "    scores.append(accuracy_score(y[test_ind], clf.predict(X[test_ind])))\n",
    "score = np.mean(np.array(scores))\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Умная инициализация не дала улучшения качества модели(вероятно нет простых трендов), однако очень сильно увеличила скорость обучения на SVM. Если применять умную инициализацию, то только с моделями, которые быстро предсказывают результат(например, линейной). Если использовать деревянную начальную модель, то можно переобучиться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения  ансамблей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ваш ответ здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Здесь Вы можете оставить отзыв о этой домашней работе или о всем курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ВАШ ОТЗЫВ ЗДЕСЬ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
